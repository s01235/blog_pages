{"meta":{"title":"Jian's Blog","subtitle":"","description":"","author":"SongChenjian","url":"http://cj_song.gitee.io/myblogs","root":"/myblogs/"},"pages":[{"title":"404 Not Found","date":"2020-08-13T10:47:36.295Z","updated":"2020-08-06T08:02:32.253Z","comments":true,"path":"404.html","permalink":"http://cj_song.gitee.io/myblogs/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"关于","date":"2020-08-13T10:53:15.655Z","updated":"2020-08-13T10:53:08.083Z","comments":true,"path":"about/index.html","permalink":"http://cj_song.gitee.io/myblogs/about/index.html","excerpt":"","text":"name宋晨健 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"所有分类","date":"2020-08-13T10:53:15.658Z","updated":"2020-08-13T10:52:37.004Z","comments":true,"path":"categories/index.html","permalink":"http://cj_song.gitee.io/myblogs/categories/index.html","excerpt":"","text":"test document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"contact","date":"2020-08-06T07:20:00.000Z","updated":"2020-08-06T07:24:08.792Z","comments":true,"path":"contact/index.html","permalink":"http://cj_song.gitee.io/myblogs/contact/index.html","excerpt":"","text":"test document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"","date":"2020-08-13T10:47:36.321Z","updated":"2020-07-11T06:22:21.000Z","comments":true,"path":"mylist/index.html","permalink":"http://cj_song.gitee.io/myblogs/mylist/index.html","excerpt":"","text":"test document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"我的朋友们","date":"2020-08-13T11:11:51.428Z","updated":"2020-08-13T11:11:15.953Z","comments":true,"path":"friends/index.html","permalink":"http://cj_song.gitee.io/myblogs/friends/index.html","excerpt":"这里写友链上方的内容。","text":"这里写友链上方的内容。 这里可以写友链页面下方的文字备注，例如自己的友链规范、示例等。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"},{"title":"所有标签","date":"2020-08-13T11:12:27.147Z","updated":"2020-08-13T11:11:33.182Z","comments":true,"path":"tags/index.html","permalink":"http://cj_song.gitee.io/myblogs/tags/index.html","excerpt":"","text":"test document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });"}],"posts":[{"title":"Adam论文翻译","slug":"blog2_Adam论文翻译","date":"2020-08-13T10:15:00.000Z","updated":"2020-08-13T11:23:31.078Z","comments":true,"path":"2020/08/13/blog2-adam-lun-wen-fan-yi/","link":"","permalink":"http://cj_song.gitee.io/myblogs/2020/08/13/blog2-adam-lun-wen-fan-yi/","excerpt":"本文为Adam算法原论文的翻译。","text":"本文为Adam算法原论文的翻译。 第一部分：引言​ 基于随机梯度的优化问题在许多科学和工程领域具有重要的实际意义。这些领域中的许多问题都可以归结为某个标量参数化目标函数的优化问题，这些目标函数要求对其参数进行最大化或最小化。如果函数的参数是可微的，梯度下降法是一种比较有效的优化方法，因为计算一阶偏导数w.r.t.的所有参数都与计算函数的计算复杂度相同。通常，目标函数是随机的。在这种情况下，可以用多个随机子函数组成的梯度函数或多个梯度子函数组成。SGD被证明是一种高效、有效的优化方法，在许多机器学习成功案例中起着核心作用，例如深度学习的最新进展（Deng等人，2013；Krizhevsky等人，2012；Hinton&amp;Salakhutdinov，2006；Hinton et al.，2012a；Graves et al.，2013）。目标也可能有数据子采样以外的其他噪声源，如辍学（Hinton等人，2012b）正则化。对于所有这些噪声目标，需要有效的随机优化技术。本文主要研究具有高维参数空间的随机目标优化问题。在这些情况下，高阶优化方法不适合，本文的讨论将局限于一阶方法。 ​ 我们提出了Adam，一种只需要一阶梯度且内存需求很少的高效随机优化方法。该方法根据梯度的一阶矩和二阶矩的估计来计算不同参数的个体自适应学习率；Adam的名字来源于自适应矩估计。我们的方法结合了最近流行的两种方法的优点：AdaGrad（Duchi et al.，2011）和RMSProp（Tieleman&amp;Hinton，2012），这两种方法在在线和非平稳环境下工作良好；第5节阐明了这些方法和其他随机优化方法的重要联系。Adam的一些优点是参数更新的大小对梯度的重定标是不变的，它的步长是由步长超参数近似限定的，它不需要一个平稳的目标，它与稀疏梯度一起工作，并且它自然地执行一种步长退火的形式。 算法1： $g_t^2$代表元素平方$g_t \\bigodot g_t$，被测试机器学习问题的良好的默认设置参数为$\\alpha = 0.001, \\beta_1=0.9,\\beta_2=0.99$ 以及$\\epsilon=10^{-8}$。对向量所有的操作都是基于元素的。对于$\\beta_1^t 与\\beta_2^t$表示$\\beta_1 \\beta_2$的t次方。 $ \\bf{Require: } \\quad\\alpha:\\; Stepsize$ $\\bf{Require:} \\quad\\beta_1,\\beta_2\\in[0,1): \\; Exponential\\;decay\\; rates\\; for\\; the\\; moment\\; estimates$ $ \\bf{Require: } \\quad f(\\theta):\\; Stochastic\\; objective\\; function\\; with\\; parameters\\; \\theta $ $ \\bf{Require: } \\quad\\theta_0:\\; Initial\\; parameter\\; vector $ $\\qquad m_0 \\leftarrow 0 (Initialize\\; 1^{st} \\;moment \\; vector) $ $\\qquad v_0 \\leftarrow 0 (Initialize\\; 2^{nd} \\;moment \\; vector) $ $\\qquad t \\leftarrow 0 (Initialize\\; timestep) $ $\\qquad \\bf{while} \\;\\theta_t \\; not\\; converged \\; \\bf{do} $ $\\qquad \\qquad t \\leftarrow t+1 $ $\\qquad \\qquad g_t \\leftarrow \\nabla_{\\theta}f_t(\\theta_t-1) \\;(Get\\; gradients\\; w.r.t.\\; stochastic\\; objective\\; at\\; timestep\\; t) $ $\\qquad \\qquad m_t \\leftarrow \\beta_1 \\cdot m_{t-1} +(1-\\beta_1)\\cdot g_t \\;(Update\\; biased\\; first\\; moment\\; estimate) $ $\\qquad \\qquad v_t \\leftarrow \\beta_2 \\cdot v_{t-1} +(1-\\beta_2)\\cdot g_t^2 \\;(Update\\; biased\\; second\\; raw\\; moment\\; estimate) $ $\\qquad \\qquad \\hat{m_t} \\leftarrow m_t/(1-\\beta_1^t) \\;(Compute\\; bias-corrected\\; first\\; moment\\; estimate) $ $\\qquad \\qquad \\hat{v_t} \\leftarrow v_t/(.1-\\beta_2^t) \\;(Compute\\; bias-corrected\\; second\\; raw \\; moment\\; estimate) $ $\\qquad \\qquad \\theta_t \\leftarrow \\theta{t-1} - \\alpha\\cdot \\hat{m_t}/(\\sqrt{\\hat v_t} +\\epsilon ) \\;(Update \\; parameters)$ $\\qquad \\bf{end\\; while}$ $\\qquad \\bf{return} \\; \\theta_t \\;(Resulting \\; parameters) $ ​ 第二节介绍了算法及其更新规则的性质。第3节介绍了我们的初始化偏差校正技术，第4节对在线凸规划中的Adam收敛性进行了理论分析。从经验上讲，我们的方法在各种模型和数据集上始终优于其他方法，如第6节所示。总的来说，我们证明了Adam是一个多功能的算法，可以扩展到大规模的高维机器学习问题。 第二部分：算法​ 我们提出的算法Adam的伪代码见算法1。设$f(\\theta)$是一个有噪声的目标函数：一个可微$w.r.t.$参数$ \\theta$的随机标量函数。我们感兴趣的是最小化这个函数的期望值，$\\mathbb{E}[f(\\theta)]\\,w.r.t. $它的参数$\\theta$。用$f_1(\\theta), \\cdots, f_T(\\theta) $表示随机函数在随后的时间步骤$1，\\cdots，T$的实现。随机性可能来自于对数据点的随机子样本（小批量）的评估，也可能来自固有的函数噪声。用$g_t=\\bigtriangledown_{\\theta}f_t(\\theta)$表示梯度，即在时间步长t处计算的$f_t，w.r.t\\;\\theta$的偏导数向量。 ​ 该算法更新梯度$m_t$和平方梯度$v_t$的指数移动平均，其中超参数$β_1，β_2\\in[0，1）$控制这些移动平均的指数衰减率。移动平均值本身就是梯度的第一个矩（平均值）和第二个原始矩（无中心方差）的估计值。然而，这些移动平均值被初始化为（向量）0，导致矩估计值偏向于零，特别是在初始时间步长期间，尤其是当衰减率很小（即$\\beta_s$接近1）时。好消息是，这种初始化偏差可以很容易地抵消，从而得到偏差校正的估计值$\\hat m_t$和$\\hat v_t$。有关更多详细信息，请参阅第3节。 ​ 注意，算法1的效率可以通过改变计算顺序（例如，用下面的行替换循环中的最后三行）来提高算法1的效率: \\alpha_t=\\frac {\\alpha \\cdot \\sqrt{1-\\beta^t_2}}{(1-\\beta^t_1)} \\theta_t = \\theta_{t-1}-\\frac{\\alpha_t\\cdot m_t}{(\\sqrt{v_t}+\\hat \\epsilon )}2.1 Adam更新率​ Adam更新规则的一个重要属性是它对步长的谨慎选择。假设$\\epsilon =0$，时间步长t在参数空间中采取的有效步骤为$\\Delta_t=\\alpha\\cdot \\frac{\\hat m_t}{\\sqrt{\\hat v_t}}. $ 有效步长有两个上界：在$(1-\\beta_1)&gt;\\sqrt{1-\\beta_2}$的情况下$\\left|\\Delta_{t}\\right| \\leq \\alpha \\cdot\\left(1-\\beta_{1}\\right) / \\sqrt{1-\\beta_{2}} $,以及$\\left|\\Delta_{t}\\right| \\leq \\alpha$。否则,第一种情况只发生在最严重的稀疏情况下：梯度在除当前时间步以外的所有时间步上都为零。对于较少稀疏的情况，有效步长将更小。当$\\left(1-\\beta_{1}\\right)=\\sqrt{1-\\beta_{2}}$情况下，有$\\left|\\hat{m}_{t} / \\sqrt{\\hat{v}_{t}}\\right|&lt;1$因此$\\left|\\Delta_{t}\\right|&lt;\\alpha$。在更普通的场景中，因为$|\\mathbb{E}[g] / \\sqrt{\\mathbb{E}\\left[g^{2}\\right]}| \\leq 1$我们有$\\widehat{m}_{t} / \\sqrt{\\widehat{v}_{t}} \\approx \\pm 1$ .每个时间步在参数空间中所采取的有效步长近似为步长设置α的范围，即$\\left|\\Delta_{t}\\right| \\leqq \\alpha$。这可以理解为在当前参数值周围建立一个信任区域，超过该区域，当前梯度估计无法提供足够的信息。这通常使得提前知道α的正确刻度相对容易。例如，对于许多机器学习模型，我们通常预先知道好的最优解在参数空间的某个集合区域内具有高概率；例如，对参数具有先验分布的情况并不少见。由于α在参数空间中设置了步长（的上界），我们通常可以推导出α的正确量级，从而在一定的迭代次数内从$\\theta_0$到达最优值。稍加滥用术语，我们将比率$\\hat{m}_{t} / \\sqrt{\\widehat{v}_{t}}$称为信噪比$（SNR）$。信噪比越小，有效步长$∆t$将更接近于零。这是一个理想的特性，因为较小的信噪比意味着$\\hat m_t$的方向是否与真实梯度的方向相对应存在更大的不确定性。例如，$SNR$值通常会向最优值靠拢，从而导致参数空间中的有效步长更小：这是自动退火的一种形式。有效步长$∆t$也与梯度的比例不变性；用系数c重新缩放梯度g将用系数c缩放$\\hat{m}_{t}$，用系数$c^2$缩放$\\hat{v}_{t}$这抵消了$\\hat{m}_{t} / \\sqrt{\\widehat{v}_{t}}\\left(c \\cdot \\hat{m}_{t}\\right) /(\\sqrt{c^{2} \\cdot \\hat{v}_{t}})=\\hat{m}_{t} / \\sqrt{\\hat{v}_{t}}$。 第三部分：初始化偏差校正​ 如第2节所述，Adam使用初始化偏差校正项。我们将在这里导出二阶矩估计的项；一阶矩估计的推导完全类似。设g是随机目标f的梯度，我们希望用平方梯度的指数移动平均估计其第二原始矩（无中心方差），衰减率为$β2$。设$g_1，\\cdots，g_T$为后续时间步的梯度，每个都是从基本梯度分布$g_t∼p(gT)$中提取的。让我们将指数移动平均值初始化为$v_0=0$（零向量）。首先注意指数移动平均值$v_{t}=\\beta_{2} \\cdot v_{t-1}+\\left(1-\\beta_{2}\\right) \\cdot g_{t}^{2}$（其中$g_t^2$表明了元素平方$g_t\\odot g_t$）在时间步长t处的更新可以写成所有先前时间步长的梯度函数： v_{t}=\\left(1-\\beta_{2}\\right) \\sum_{i=1}^{t} \\beta_{2}^{t-i} \\cdot g_{i}^{2} \\tag{1}​ 我们想知道时间步长t处指数移动平均值的期望值$\\mathbb{E}\\left[v_{t}\\right]$是如何与真实的第二时刻$\\mathbb{E}\\left[g_{t}^2\\right]$相关的，因此我们可以纠正两者之间的差异。取等式（1）左右两侧的期望值： \\begin{aligned} \\mathbb{E}\\left[v_{t}\\right] &=\\mathbb{E}\\left[\\left(1-\\beta_{2}\\right) \\sum_{i=1}^{t} \\beta_{2}^{t-i} \\cdot g_{i}^{2}\\right] \\\\ &=\\mathbb{E}\\left[g_{t}^{2}\\right] \\cdot\\left(1-\\beta_{2}\\right) \\sum_{i=1}^{t} \\beta_{2}^{t-i}+\\zeta \\\\ &=\\mathbb{E}\\left[g_{t}^{2}\\right] \\cdot\\left(1-\\beta_{2}^{t}\\right)+\\zeta \\end{aligned}​ 式中：$ζ=0$，如果真实第二力矩$\\mathbb{E}\\left[g_{i}^2\\right]$是静止的；否则，$ζ$可以保持较小，因为指数衰减率$β1$可以（并且应该）被选择为指数移动平均为过去太远的梯度分配小权重。剩下的是项$（1−β_t)$，这是由用零初始化运行平均值引起的。因此，在算法1中，我们用这个项来修正初始化偏差。 ​ 在稀疏梯度的情况下，为了可靠地估计二阶矩，需要通过选择较小的$β2$值对许多梯度进行平均；然而，正是在小$β2$的情况下，缺少初始化偏差校正将导致初始步骤大得多。 第四部分：收敛性分析​ 我们使用（Zinkevich，2003）提出的在线学习框架来分析Adam的收敛性。给定凸代价函数$f_1（θ），f_2（θ），\\cdots，f_T（θ）$的任意未知序列。在每个时刻$t$，我们的目标是预测参数$θ_t$，并在先前未知的代价函数$f_t$上对其进行评估。由于序列的性质是预先未知的，因此我们使用遗憾来评估我们的算法，即在线预测$f_t(θt)$与来自前面所有步骤的可行集$\\mathcal{X}$。具体地说，遗憾是指： R(T)=\\sum_{t=1}^{T}\\left[f_{t}\\left(\\theta_{t}\\right)-f_{t}\\left(\\theta^{*}\\right)\\right] \\tag{5}​ 其中$\\theta^{*}=\\arg \\min _{\\theta \\in \\mathcal{X}} \\sum_{t=1}^{T} f_{t}(\\theta)$。我们证明Adam没有遗憾边界，并在附录中给出了证明。我们的结果与一般凸在线学习问题的已知界相当。我们还使用一些定义来简化我们的符号，其中$g_{t} \\triangleq \\nabla f_{t}\\left(\\theta_{t}\\right)$和$g_{t,i}$作为第i个元素。我们将$g_{1: t, i} \\in \\mathbb{R}^{t}$定义为一个向量，它包含所有迭代过程中梯度的第i维，直到t，$g_{1: t, i}=\\left[g_{1, i}, g_{2, i}, \\cdots, g_{t, i}\\right]$。同样，我们定义$\\gamma \\triangleq \\frac{\\beta_{1}^{2}}{\\sqrt{\\beta_{2}}}$。当学习速率$α_t$以$t-1$的速率衰减时，我们的以下定理成立，并且第一时刻平均系数$β_{1,t}$以$λ$指数衰减，通常接近1，例如$1-10^{-8}$。 定理4.1. 假设函数ft具有有界梯度，$\\left|\\nabla f_{t}(\\theta)\\right|_{2} \\leq G,\\left|\\nabla f_{t}(\\theta)\\right|_{\\infty} \\leqq G_{\\infty}$对于所有的$\\theta \\in R^{d }$并且Adam生成的任何$θt$之间的距离是有界的，$\\left|\\theta_{n}-\\theta_{m}\\right|_{2} \\leq D, \\left|\\theta_{m}-\\theta_{n}\\right|_{\\infty} \\leq D_{\\infty}$对于任何$m, n \\in\\{1, \\ldots, T\\}, \\text { and } \\beta_{1}, \\beta_{2} \\in[0,1)$满足$\\frac{\\beta_{1}^{2}}{\\sqrt{\\beta_{2}}}&lt;1$，让$\\alpha_{t}=\\frac{\\alpha}{\\sqrt{t}}$并且$\\beta_{1, t}=\\beta_{1} \\lambda^{t-1}, \\lambda \\in(0,1)$。Adam对于所有$T≥1$达到以下保证。 R(T) \\leq \\frac{D^{2}}{2 \\alpha\\left(1-\\beta_{1}\\right)} \\sum_{i=1}^{d} \\sqrt{T \\hat{v}_{T, i}}+\\frac{\\alpha\\left(1+\\beta_{1}\\right) G_{\\infty}}{\\left(1-\\beta_{1}\\right) \\sqrt{1-\\beta_{2}}(1-\\gamma)^{2}} \\sum_{i=1}^{d}\\left\\|g_{1: T, i}\\right\\|_{2}+\\sum_{i=1}^{d} \\frac{D_{\\infty}^{2} G_{\\infty} \\sqrt{1-\\beta_{2}}}{2 \\alpha\\left(1-\\beta_{1}\\right)(1-\\lambda)^{2}} 我们的定理4.1表明，当数据特征是稀疏且有界梯度时，求和项可以远小于其上限$\\sum_{i=1}^{d}\\left|g_{1: T, i}\\right|_{2}&lt;&lt;d G_{\\infty} \\sqrt{T}$并且$\\sum_{i=1}^{d} \\sqrt{T \\widehat{v}_{T, i}}&lt;&lt;d G_{\\infty} \\sqrt{T}$,尤其是如果函数和数据特征的类别是第1.2节(Duchi等人，2011）的形式。他们对期望值$\\mathbb{E}\\left[\\sum_{i=1}^{d}\\left|g_{1: T, i}\\right|_{2}\\right]$的结果也适用于Adam。通常，类似于Adam与Adagrad的自适应方法，可以实现$O(\\log d \\sqrt{T})$相比于非自适应方法计算复杂度$O(\\sqrt{d T})$的改进。在我们的理论分析中，将$β_{1,t}$衰减为零是很重要的，也与以前的经验结果相吻合，例如（Sutskever等人，2013年）表明在训练结束时降低动量系数可以提高收敛性。最后，我们可以展示Adam的平均遗憾. 推论4.2.假设函数$f_t$具有有界梯度，$\\left|\\nabla f_{t}(\\theta)\\right|_{2} \\leq G,\\left|\\nabla f_{t}(\\theta)\\right|_{\\infty} \\leq G_{\\infty} \\text { for all } \\theta \\in R^{d}$并且Adam生成的任何$θt$之间的距离是有界的，$\\left|\\theta_{n}-\\theta_{m}\\right|_{2} \\leq D,\\left|\\theta_{m}-\\theta_{n}\\right|_{\\infty} \\leq D_{\\infty}$对任意的$m, n \\in\\{1, \\ldots, T\\}$。Adam达到以下保证，对于所有$T≥1$。 \\frac{R(T)}{T}=O\\left(\\frac{1}{\\sqrt{T}}\\right)利用定理4.1和$\\sum_{i=1}^{d}\\left|g_{1: T, i}\\right|_{2} \\leq d G_{\\infty} \\sqrt{T}$可以得到这个结果。因此，$\\lim _{T \\rightarrow \\infty} \\frac{R(T)}{T}=0$。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[],"tags":[{"name":"Adam, 优化算法","slug":"Adam-优化算法","permalink":"http://cj_song.gitee.io/myblogs/tags/Adam-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"}]},{"title":"Windows下安装hexo并部署网站","slug":"blog1_安装hexo以及网站的部署","date":"2020-08-04T14:00:00.000Z","updated":"2020-08-05T02:27:01.164Z","comments":true,"path":"2020/08/04/blog1-an-zhuang-hexo-yi-ji-wang-zhan-de-bu-shu/","link":"","permalink":"http://cj_song.gitee.io/myblogs/2020/08/04/blog1-an-zhuang-hexo-yi-ji-wang-zhan-de-bu-shu/","excerpt":"本文主要介绍在Windows操作系统上使用hexo生成个人博客的软件安装及环境配置，同时记录了过程中用到的相关命令。","text":"本文主要介绍在Windows操作系统上使用hexo生成个人博客的软件安装及环境配置，同时记录了过程中用到的相关命令。 一、安装node.jswindows版本的官网链接为：https://nodejs.org/en/ ，安装过程中CustomSetup这一步记得检查Add—to-PATH是否被选上。 安装完成后命令行输入以下命令检查环境是否配好，正常情况下会返回软件版本号。 npm -v 二、安装hexo在Windows的cmd命令行中输入以下命令安装hexo并验证。 npm install -g hexo hexo -v 如果出现以下错误： hexo : 无法加载文件 C:\\Users\\45478\\AppData\\Roaming\\npm\\hexo.ps1，因为在此系统上禁止运行脚本。 可在管理员模式下的powershell中执行以下命令，输入y回车，之后问题解决。 set-ExecutionPolicy RemoteSigned 三、生成博客步骤为创建工作目录，工作目录初始化，生成博客，开启本地服务预览网站。指令代码如下。 hexo init hexo generate hexo server 四、部署网站 首先执行以下命令安装hexo-deployer-git。 npm install hexo-deployer-git --save 网站的部署可以参考hexo的官方教程，主要用到的命令如下。 hexo deploy deploy之前需要更改_config.yml中deploy段的参数，可使用多个deployer。 deploy: - type: git repo: - type: heroku repo: 修改配置。 deploy: type: git repo: &lt;repository url&gt; #https://bitbucket.org/JohnSmith/johnsmith.bitbucket.io branch: [branch] message: [message] 生成站点文件并推送至远程库。执行以下指令hexo clean hexo deploy 五、HEXO主题可以在官网的主题页面找适合自己的主题，本文所用的主题为volantis，在此对主题的作者表示崇高的敬意！！! document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[],"tags":[{"name":"hexo, 部署网站","slug":"hexo-部署网站","permalink":"http://cj_song.gitee.io/myblogs/tags/hexo-%E9%83%A8%E7%BD%B2%E7%BD%91%E7%AB%99/"}]},{"title":"Hello World","slug":"hello-world","date":"2020-07-22T07:25:31.423Z","updated":"2020-07-11T07:28:00.000Z","comments":true,"path":"2020/07/22/hello-world/","link":"","permalink":"http://cj_song.gitee.io/myblogs/2020/07/22/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new \"My New Post\" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"Adam, 优化算法","slug":"Adam-优化算法","permalink":"http://cj_song.gitee.io/myblogs/tags/Adam-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"},{"name":"hexo, 部署网站","slug":"hexo-部署网站","permalink":"http://cj_song.gitee.io/myblogs/tags/hexo-%E9%83%A8%E7%BD%B2%E7%BD%91%E7%AB%99/"}]}